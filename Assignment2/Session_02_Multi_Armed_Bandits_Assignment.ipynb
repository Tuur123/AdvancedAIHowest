{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2 - Multi-Armed Bandits - Assignment\n",
    "\n",
    "In this assignment you will apply different multi-armed bandit algorithms in order to discover the bandit with highest reward.\n",
    "We'll start with deterministic bandits, meaning that the bandit rewards are not subject to any underlying stochastic process.\n",
    "Next we'll focuss on stochastic bandits where the reward of a bandit is the result of a stochastic process and therefore making the discovery of the best bandit much harder. \n",
    "Finally we will applay the MAB strategies to solve practical real-life problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Deterministic bandits\n",
    "Assume you have to play 5 bandits with fixed rewards. These rewards are hidden and unknown when you start playing. \n",
    "You can play the bandits 100 times. Find out how you can maximize your total reward over these 100 trials.\n",
    "\n",
    "Use the BanditEnv_1 environment for this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Deterministic Bandit Environment\n",
    "\n",
    "class BanditEnv_1:\n",
    "    def __init__(self):\n",
    "        self.rewards = [-10,6,8,0,-2]\n",
    "        \n",
    "    def reset(self):\n",
    "        self.rewards = [-10,6,8,0,-2]\n",
    "        \n",
    "    def step(self,action):\n",
    "        self.action = action\n",
    "        return self.rewards[self.action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Env1 object\n",
    "Env1 = BanditEnv_1()\n",
    "Env1.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward = -10\n"
     ]
    }
   ],
   "source": [
    "# Taken an action. For example choose bandit 0\n",
    "reward = Env1.step(0)\n",
    "print('reward =', reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you know in advance that the bandits have fixed rewards, the best strategy will be to play them all once and then pick the one with the highest reward (greedy action)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-10, 6, 8, 0, -2]\n",
      "Bandit with the highest reward is bandit 2 with a reward of 8\n",
      "The total reward =  710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd53e84c400>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards = []\n",
    "totalReward = 0\n",
    "for action in range(5):\n",
    "    rewards.append(Env1.step(action))\n",
    "    totalReward = totalReward + reward\n",
    "    \n",
    "print(rewards)\n",
    "bestAction = np.argmax(rewards)\n",
    "print('Bandit with the highest reward is bandit',bestAction, 'with a reward of',rewards[bestAction])\n",
    "\n",
    "for i in range(100 - 5):\n",
    "    rewards.append(Env1.step(bestAction))\n",
    "    totalReward = totalReward + Env1.step(bestAction)\n",
    "\n",
    "print('The total reward = ', totalReward)\n",
    "\n",
    "# Plot the rewards as a funtion of the number of trials\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Stochastic bandits\n",
    "\n",
    "In the case of stochastic bandits, the reward is sampled from an initially unknown distribution. Our task is to discover the expected reward of each of the bandits as quickly (and reliably) as possible. \n",
    "As an example, assume that the reward of a bandit comes from a normal distribution with mean = 10 and standard deviation of 5. This means that in 68% procent of the trials this bandit will return a reward between 10-5 = 5 and 10+5 = 15. If you play this bandit multiple times, you can expect that the average reward will converge to 10.\n",
    "\n",
    "Use BanditEnv_2 for this exercise. All 5 bandits have rewards coming from normal distributions with different means, but with the same standard deviation (std = 1).\n",
    "You can play this bandit 200 times. Try to come up with a good strategy to maximize the total reward over these 200 trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditEnv_2:\n",
    "    def __init__(self):\n",
    "        self.means = [-10,6,8,0,-2]\n",
    "        self.std = 1\n",
    "        \n",
    "    def reset(self):\n",
    "        self.means = [-10,6,8,0,-2]\n",
    "        self.std = 1\n",
    "\n",
    "    def step(self,action):\n",
    "        self.action = action\n",
    "        return np.random.normal(self.means[self.action], self.std, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-10, 6, 8, 0, -2]\n"
     ]
    }
   ],
   "source": [
    "# Create an Env2 object\n",
    "Env2 = BanditEnv_2()\n",
    "Env2.reset()\n",
    "print(Env2.means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Epsilon Greedy\n",
    "\n",
    "1. Implement the epsilon greedy algorithm to play the Env2 bandits. \n",
    "2. Show the influence of epsilon parameter. Do this by plotting the reward history for different values of epsilon. Also visualize how often a bandit was chosen.\n",
    "3. Find the optimal epsilon value giving you maximum total reward.\n",
    "4. Use epsilon decay. This means that you gradually decrease the value of epsilon as the number of trials increases. Use the following rule: $\\epsilon(t+1) = \\eta \\times \\epsilon(t) $ where $0< \\eta < 1$\n",
    "5. Now test for different values of the standard deviation. What if the standard deviation becomes larger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon greedy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Optimistic Initial Value\n",
    "\n",
    "Continue to work with the Env2 environment. \n",
    "Implement the Optimistic Initial Value algorithm. Start by initializing the estimated bandit means to a high non-zero value and use a greedy approach. \n",
    "After each play the estimated bandit mean will come closer to the true bandit mean.\n",
    "\n",
    "1. Implement the optimistic initial value algorithm to play the Env2 bandits. Check the final values of the estimated means.\n",
    "2. Show the initialization. What happens if you initialize the estimated means to larger values? \n",
    "3. Now test for different values of the standard deviation. What if the standard deviation becomes larger?\n",
    "4. Compare the optimistic initial value approach to the epsilon greedy approach. Plot the reward history of both in one graph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimistic initial value\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Upper Confidence Bound\n",
    "\n",
    "Continue to work with the Env2 environment and implement the Upper Confidence Bound algorithm. \n",
    "Select the action based on the following rule:\n",
    "\n",
    "$A_t = argmax_a \\left( MeanRewards + c  \\sqrt{\\frac{\\ln{t}}{N_a(t)}}  \\right)$\n",
    "\n",
    "1. Implement the upper confidence bound algorithm to play the Env2 bandits. Check the final values of the estimated means.\n",
    "2. Change the parameter c? Explain the influence of c. \n",
    "3. Now test for different values of the standard deviation. What if the standard deviation becomes larger?\n",
    "4. Compare the upper confidence bound approach to the epsilon greedy approach and opimistic initial value. Plot the reward histories of the algorithms in one graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upper Confidence Bound\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRA: non-stationary bandits\n",
    "\n",
    "In the following you have to deal with non-stationary bandits. This means that the expected reward is not stationary and will change over time. \n",
    "in the BanditEnv_3, the initial expected rewards will change with a constant after each action.\n",
    "\n",
    "Figure out the best strategy to optimize the total reward after 200 actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditEnv_3:\n",
    "    def __init__(self):\n",
    "        self.means = np.array([[-10,6,8,0,-2]])\n",
    "        self.std = 1\n",
    "    def reset(self):\n",
    "        self.means = np.array([[-10,6,8,0,-2]])\n",
    "        \n",
    "    def step(self,action):\n",
    "        self.action = action\n",
    "        self.means = self.means + np.array([[0.15,-0.1,-0.15,0.05,0.1]])\n",
    "        return np.random.normal(self.means[0,self.action], self.std, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an env3 environment\n",
    "env3 = BanditEnv_3()\n",
    "env3.reset()\n",
    "\n",
    "# solve the environment (for 200 steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Server latencies\n",
    "\n",
    "You have to make 1000 webpage requests. There are 25 servers with different latencies to choose from. \n",
    "Your task is to come up with a clever strategy to reduce the cumulative latency of these 1000 webpage requests.\n",
    "The file 'latencies.csv' contains these 1000 different request. \n",
    "It's important to know that in a practical situation you would not have access to this data and requests will be made sequentially.\n",
    "Use a multi-armed bandit to find the best strategy to minimize the total latency over these 1000 requests. Again, it's not allowed to process and statistically analyse the data in advance.\n",
    "\n",
    "1. Try and optimize different types of bandits: epsilon-greedy, optimistic initial value, upper confidence bound. \n",
    "2. Compare these bandits in terms of total cumulated reward (or regret). Make a graph to visualize this.\n",
    "3. Make graphs showing how often each bandit was chosen.\n",
    "\n",
    "\n",
    "EXTRA: program a bayesian badit to solve the latency problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Server_1</th>\n",
       "      <th>Server_2</th>\n",
       "      <th>Server_3</th>\n",
       "      <th>Server_4</th>\n",
       "      <th>Server_5</th>\n",
       "      <th>Server_6</th>\n",
       "      <th>Server_7</th>\n",
       "      <th>Server_8</th>\n",
       "      <th>Server_9</th>\n",
       "      <th>Server_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Server_16</th>\n",
       "      <th>Server_17</th>\n",
       "      <th>Server_18</th>\n",
       "      <th>Server_19</th>\n",
       "      <th>Server_20</th>\n",
       "      <th>Server_21</th>\n",
       "      <th>Server_22</th>\n",
       "      <th>Server_23</th>\n",
       "      <th>Server_24</th>\n",
       "      <th>Server_25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>265</td>\n",
       "      <td>1080</td>\n",
       "      <td>136</td>\n",
       "      <td>25</td>\n",
       "      <td>395</td>\n",
       "      <td>1350</td>\n",
       "      <td>105</td>\n",
       "      <td>113</td>\n",
       "      <td>85</td>\n",
       "      <td>471</td>\n",
       "      <td>...</td>\n",
       "      <td>414</td>\n",
       "      <td>129</td>\n",
       "      <td>410</td>\n",
       "      <td>176</td>\n",
       "      <td>589</td>\n",
       "      <td>92</td>\n",
       "      <td>56</td>\n",
       "      <td>1298</td>\n",
       "      <td>234</td>\n",
       "      <td>2603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>473</td>\n",
       "      <td>6306</td>\n",
       "      <td>84</td>\n",
       "      <td>121</td>\n",
       "      <td>392</td>\n",
       "      <td>33</td>\n",
       "      <td>103</td>\n",
       "      <td>97</td>\n",
       "      <td>876</td>\n",
       "      <td>883</td>\n",
       "      <td>...</td>\n",
       "      <td>451</td>\n",
       "      <td>129</td>\n",
       "      <td>412</td>\n",
       "      <td>245</td>\n",
       "      <td>1113</td>\n",
       "      <td>132</td>\n",
       "      <td>62</td>\n",
       "      <td>11429</td>\n",
       "      <td>305</td>\n",
       "      <td>2264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>269</td>\n",
       "      <td>1064</td>\n",
       "      <td>83</td>\n",
       "      <td>25</td>\n",
       "      <td>399</td>\n",
       "      <td>33</td>\n",
       "      <td>104</td>\n",
       "      <td>99</td>\n",
       "      <td>615</td>\n",
       "      <td>639</td>\n",
       "      <td>...</td>\n",
       "      <td>417</td>\n",
       "      <td>132</td>\n",
       "      <td>561</td>\n",
       "      <td>143</td>\n",
       "      <td>652</td>\n",
       "      <td>101</td>\n",
       "      <td>56</td>\n",
       "      <td>953</td>\n",
       "      <td>232</td>\n",
       "      <td>2293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>270</td>\n",
       "      <td>1071</td>\n",
       "      <td>84</td>\n",
       "      <td>24</td>\n",
       "      <td>393</td>\n",
       "      <td>33</td>\n",
       "      <td>120</td>\n",
       "      <td>118</td>\n",
       "      <td>298</td>\n",
       "      <td>755</td>\n",
       "      <td>...</td>\n",
       "      <td>415</td>\n",
       "      <td>129</td>\n",
       "      <td>327</td>\n",
       "      <td>871</td>\n",
       "      <td>670</td>\n",
       "      <td>109</td>\n",
       "      <td>60</td>\n",
       "      <td>954</td>\n",
       "      <td>232</td>\n",
       "      <td>2251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>296</td>\n",
       "      <td>1065</td>\n",
       "      <td>84</td>\n",
       "      <td>25</td>\n",
       "      <td>395</td>\n",
       "      <td>34</td>\n",
       "      <td>104</td>\n",
       "      <td>111</td>\n",
       "      <td>83</td>\n",
       "      <td>751</td>\n",
       "      <td>...</td>\n",
       "      <td>413</td>\n",
       "      <td>130</td>\n",
       "      <td>305</td>\n",
       "      <td>122</td>\n",
       "      <td>763</td>\n",
       "      <td>102</td>\n",
       "      <td>74</td>\n",
       "      <td>1640</td>\n",
       "      <td>352</td>\n",
       "      <td>2530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Server_1  Server_2  Server_3  Server_4  Server_5   Server_6  Server_7  \\\n",
       "995       265      1080       136        25       395       1350       105   \n",
       "996       473      6306        84       121       392         33       103   \n",
       "997       269      1064        83        25       399         33       104   \n",
       "998       270      1071        84        24       393         33       120   \n",
       "999       296      1065        84        25       395         34       104   \n",
       "\n",
       "     Server_8  Server_9  Server_10  ...  Server_16  Server_17  Server_18  \\\n",
       "995       113        85        471  ...        414        129        410   \n",
       "996        97       876        883  ...        451        129        412   \n",
       "997        99       615        639  ...        417        132        561   \n",
       "998       118       298        755  ...        415        129        327   \n",
       "999       111        83        751  ...        413        130        305   \n",
       "\n",
       "     Server_19  Server_20  Server_21  Server_22  Server_23  Server_24  \\\n",
       "995        176        589         92         56       1298        234   \n",
       "996        245       1113        132         62      11429        305   \n",
       "997        143        652        101         56        953        232   \n",
       "998        871        670        109         60        954        232   \n",
       "999        122        763        102         74       1640        352   \n",
       "\n",
       "     Server_25  \n",
       "995       2603  \n",
       "996       2264  \n",
       "997       2293  \n",
       "998       2251  \n",
       "999       2530  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('latencies.csv')\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server latencies\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
